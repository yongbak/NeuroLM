# Dead Code Reset ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„ ê°€ì´ë“œ

## ğŸ” ë¬¸ì œ ì§„ë‹¨

### Codebook ì‚¬ìš©ë¥  ê¸‰ê° í˜„ìƒ
```
Iteration 0:    100% (K-means ì´ˆê¸°í™” ì•„í‹°íŒ©íŠ¸)
Iteration 50:   88.7%
Iteration 100:  49.2%
Iteration 200:  23.8%
Validation:     11.7%
```

### ì›ì¸ ë¶„ì„

#### 1. **EMAì˜ êµ¬ì¡°ì  ë¬¸ì œ**
```python
# EMA ì—…ë°ì´íŠ¸ ê³µì‹
cluster_size_new = 0.9 * cluster_size_old + 0.1 * bins_current

# ë¬¸ì œì :
# - ì´ˆë°˜ì— ìì£¼ ì‚¬ìš©ëœ ì½”ë“œëŠ” cluster_sizeê°€ í¬ê²Œ ìœ ì§€ë¨
# - DECAY=0.9ëŠ” ê³¼ê±° 90% + í˜„ì¬ 10% â†’ ê³¼ê±° í¸í–¥ì´ ë§¤ìš° ê°•í•¨
# - í•œ ë²ˆ dominantí•´ì§„ ì½”ë“œëŠ” ê³„ì† dominantí•˜ê²Œ ìœ ì§€ë¨
```

#### 2. **ì•…ìˆœí™˜ (Vicious Cycle)**
```
1. íŠ¹ì • ì½”ë“œ(ì˜ˆ: Code 62)ê°€ ì´ˆê¸°ì— ë§ì´ ì‚¬ìš©ë¨
   â†“
2. EMAë¡œ í•´ë‹¹ ì½”ë“œì˜ ì„ë² ë”©ì´ ê°•í™”ë¨
   â†“
3. ë‹¤ì–‘í•œ ì…ë ¥ë„ í•´ë‹¹ ì½”ë“œì— ë§¤í•‘ë˜ê¸° ì‹œì‘
   â†“
4. ë‚˜ë¨¸ì§€ ì½”ë“œë“¤ì€ ì‚¬ìš© ë¹ˆë„ ê°ì†Œ
   â†“
5. cluster_sizeê°€ 0ì— ê°€ê¹Œì›Œì§ â†’ "Dead Code"
   â†“
6. Dead codeëŠ” ì—…ë°ì´íŠ¸ë˜ì§€ ì•Šì•„ ì˜ì›íˆ ì‚¬ìš© ì•ˆë¨
```

#### 3. **ì™œ ë°œìƒí•˜ëŠ”ê°€?**
- **ë†’ì€ DECAY (0.9)**: ê³¼ê±° ê°€ì¤‘ì¹˜ê°€ ë„ˆë¬´ ë†’ìŒ
- **EMAì˜ íŠ¹ì„±**: ê³¼ê±° ì •ë³´ë¥¼ ëˆ„ì í•˜ì—¬ smoothí•˜ê²Œ ì—…ë°ì´íŠ¸
- **ì´ˆê¸°í™” ë¯¼ê°ì„±**: K-means ì´ˆê¸°í™”ê°€ ë¶ˆê· í˜•í•˜ë©´ ê³„ì† ë¶ˆê· í˜• ìœ ì§€
- **No Gradient on Codebook**: EMA ë°©ì‹ì´ë¼ gradientë¡œ êµì • ë¶ˆê°€

---

## ğŸ’¡ í•´ê²°ì±…: Dead Code Reset

### í•µì‹¬ ì•„ì´ë””ì–´
ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ì½”ë“œ(dead code)ë¥¼ **í˜„ì¬ í™œì„± ìƒ˜í”Œë¡œ ì¬ì´ˆê¸°í™”**í•˜ì—¬ ë‹¤ì‹œ ê²½ìŸì— ì°¸ì—¬ì‹œí‚´

### êµ¬í˜„ ë¡œì§

```python
def reset_dead_codes(self, z_flattened, encoding_indices):
    """
    Dead codeë¥¼ í™œì„± ìƒ˜í”Œë¡œ ì¬ì´ˆê¸°í™”
    
    Args:
        z_flattened: í˜„ì¬ ë°°ì¹˜ì˜ ì¸ì½”ë” ì¶œë ¥ (N, D)
        encoding_indices: í˜„ì¬ ë°°ì¹˜ì˜ ì–‘ìí™” ì¸ë±ìŠ¤ (N,)
    """
    # 1. Dead code ì°¾ê¸°
    dead_codes = (self.cluster_size < self.dead_code_threshold).nonzero(as_tuple=True)[0]
    
    if len(dead_codes) == 0:
        return
    
    # 2. ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ì½”ë“œì˜ ìƒ˜í”Œë“¤ ì°¾ê¸°
    bins = torch.bincount(encoding_indices, minlength=self.num_tokens)
    most_used_code = bins.argmax()
    active_samples_mask = (encoding_indices == most_used_code)
    active_samples = z_flattened[active_samples_mask]
    
    # 3. Dead codeë“¤ì„ ëœë¤ ìƒ˜í”Œë¡œ ì¬ì´ˆê¸°í™”
    n_dead = len(dead_codes)
    n_samples = len(active_samples)
    
    if n_samples >= n_dead:
        indices = torch.randperm(n_samples)[:n_dead]
    else:
        indices = torch.randint(0, n_samples, (n_dead,))
    
    reset_samples = active_samples[indices]
    reset_samples = l2norm(reset_samples)  # Normalize
    
    # 4. ì„ë² ë”© ì—…ë°ì´íŠ¸
    with torch.no_grad():
        self.embedding.weight.data[dead_codes] = reset_samples
        # Cluster sizeë„ ì´ˆê¸°í™” (ì™„ì „ 0ì´ë©´ ë‹¤ì‹œ deadê°€ ë¨)
        self.cluster_size.data[dead_codes] = self.dead_code_threshold + 1.0
    
    print(f"ğŸ”„ Reset {len(dead_codes)} dead codes")
```

### í•µì‹¬ ì„¤ê³„ í¬ì¸íŠ¸

#### 1. **Dead Code íŒì • ê¸°ì¤€**
```python
dead_codes = (self.cluster_size < dead_code_threshold).nonzero()

# Threshold ì„¤ì •:
# - 0.0: ì™„ì „íˆ ì‚¬ìš©ë˜ì§€ ì•Šì€ ì½”ë“œë§Œ reset
# - 1.0: cluster_size < 1.0ì¸ ì½”ë“œ reset (ê¶Œì¥)
# - 10.0: ë” ê³µê²©ì ìœ¼ë¡œ reset (ì‚¬ìš©ë¥  ë§¤ìš° ë‚®ì€ ì½”ë“œë„ í¬í•¨)
```

#### 2. **ì–´ë–¤ ìƒ˜í”Œë¡œ ì¬ì´ˆê¸°í™”?**
- **ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ì½”ë“œì˜ ìƒ˜í”Œë“¤** ì‚¬ìš©
- ì´ìœ : í•´ë‹¹ ì½”ë“œëŠ” ê³¼ë„í•˜ê²Œ ì‚¬ìš©ë˜ê³  ìˆìœ¼ë¯€ë¡œ, ë¶„í• í•˜ì—¬ diversity í–¥ìƒ

#### 3. **ì–¸ì œ Reset?**
```python
# EMA ì—…ë°ì´íŠ¸ ì§í›„ ì‹¤í–‰
if self.training and self.embedding.update:
    # EMA update
    norm_ema_inplace(self.embedding.weight, embed_normalized, self.decay)
    
    # Dead code reset (ë§¤ iterationë§ˆë‹¤)
    self.reset_dead_codes(z_flattened, encoding_indices)
```

#### 4. **Cluster Size ì´ˆê¸°í™”**
```python
self.cluster_size.data[dead_codes] = self.dead_code_threshold + 1.0

# ì™œ threshold + 1.0?
# - 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ë©´ ë‹¤ìŒ iterationì— ë°”ë¡œ ë‹¤ì‹œ deadë¡œ íŒì •ë  ìˆ˜ ìˆìŒ
# - ì•½ê°„ì˜ "ìƒì¡´ ë²„í¼"ë¥¼ ì¤˜ì„œ ê²½ìŸ ê¸°íšŒ ì œê³µ
```

---

## ğŸš€ ì‚¬ìš© ë°©ë²•

### 1. Training ì‹œì‘
```bash
# Dead code reset í™œì„±í™” (threshold=1.0)
python train_vq.py \
    --dead_code_threshold 1.0 \
    --dataset_dir /path/to/data \
    --batch_size 32 \
    --epochs 50

# Dead code reset ë¹„í™œì„±í™”
python train_vq.py \
    --dead_code_threshold 0.0 \
    --dataset_dir /path/to/data
```

### 2. Threshold íŠœë‹ ê°€ì´ë“œ

#### Conservative (ë³´ìˆ˜ì )
```bash
--dead_code_threshold 0.0   # ì™„ì „íˆ ì‚¬ìš© ì•ˆëœ ì½”ë“œë§Œ
```
- ì¥ì : ì•ˆì •ì , ê¸°ì¡´ í•™ìŠµì— ìµœì†Œ ì˜í–¥
- ë‹¨ì : ë§¤ìš° ë‚®ì€ ì‚¬ìš©ë¥  ì½”ë“œëŠ” ì‚´ë¦¬ì§€ ëª»í•¨

#### Balanced (ê· í˜•) - **ê¶Œì¥**
```bash
--dead_code_threshold 1.0   # cluster_size < 1.0
```
- ì¥ì : ì ì ˆí•œ ê· í˜•, ëŒ€ë¶€ë¶„ì˜ ê²½ìš° íš¨ê³¼ì 
- ë‹¨ì : ë„ˆë¬´ ìì£¼ resetë˜ë©´ í•™ìŠµ ë¶ˆì•ˆì • ê°€ëŠ¥

#### Aggressive (ê³µê²©ì )
```bash
--dead_code_threshold 10.0  # cluster_size < 10.0
```
- ì¥ì : ì‚¬ìš©ë¥  ë‚®ì€ ì½”ë“œë¥¼ ì ê·¹ì ìœ¼ë¡œ ì¬í™œìš©
- ë‹¨ì : í•™ìŠµ ë¶ˆì•ˆì • ìœ„í—˜, ìˆ˜ë ´ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŒ

### 3. ëª¨ë‹ˆí„°ë§

Training ë¡œê·¸ì—ì„œ ë‹¤ìŒì„ í™•ì¸:
```
ğŸ”„ Reset 47 dead codes (cluster_size < 1.0)
   Dead codes: [5, 12, 23, 45, 67, 89, 102, 156, 203, 267, ...]
```

- **Reset ë¹ˆë„**: ì´ˆë°˜ì—ëŠ” ìì£¼, í•™ìŠµ ì§„í–‰ë˜ë©´ ê°ì†Œ ì˜ˆìƒ
- **Reset ê°œìˆ˜**: ì „ì²´ ì½”ë“œë¶ì˜ 10% ì´í•˜ê°€ ì´ìƒì 
- **íŒ¨í„´ í™•ì¸**: ê°™ì€ ì½”ë“œê°€ ë°˜ë³µì ìœ¼ë¡œ resetë˜ë©´ threshold ì¡°ì • í•„ìš”

---

## ğŸ“Š ì˜ˆìƒ íš¨ê³¼

### Before (Dead Code Reset ì—†ì´)
```
Epoch 1:  100% â†’ 88% â†’ 49% â†’ 23% (ê¸‰ê²©í•œ collapse)
Validation: 11.7%
Top code dominance: Code 62 ì‚¬ìš© 3807íšŒ (ê³¼ë„í•œ ì§‘ì¤‘)
```

### After (Dead Code Reset ì ìš©)
```
Epoch 1:  100% â†’ 92% â†’ 78% â†’ 65% (ì•ˆì •ì  ìœ ì§€)
Validation: 60%+ (ê¸°ëŒ€)
Top code dominance: ë” ê· ë“±í•œ ë¶„í¬
```

### ê°œì„  ì§€í‘œ
- âœ… **Codebook ì‚¬ìš©ë¥ **: 23.8% â†’ 60%+ (ëª©í‘œ)
- âœ… **Validation gap**: 12% â†’ 5% ì´í•˜ (train-val ì¼ì¹˜)
- âœ… **Code ì§‘ì¤‘ë„**: ê°ì†Œ (dominant codeì˜ ì‚¬ìš© ë¹ˆë„ ë‚®ì•„ì§)
- âœ… **Diversity**: Encoder ì¶œë ¥ ë‹¤ì–‘ì„± ìœ ì§€

---

## âš™ï¸ ì¶”ê°€ ê¶Œì¥ ì‚¬í•­

### 1. **DECAY ì¡°ì •ê³¼ ë³‘í–‰**
```python
# constants.py
DECAY = 0.75  # 0.9 â†’ 0.75ë¡œ ë‚®ì¶¤ (í˜„ì¬ë¥¼ ë” ë°˜ì˜)

# Dead code resetê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ì‹œë„ˆì§€
# - DECAY ë‚®ì¶”ë©´: í˜„ì¬ ë°ì´í„° ë°˜ì˜ â†‘
# - Dead code reset: ì‚¬ìš© ì•ˆë˜ëŠ” ì½”ë“œ ì¬í™œìš© â†‘
```

### 2. **BETA ì¡°ì •**
```python
# constants.py
BETA = 0.1  # 0.25 â†’ 0.1 (commitment loss ê°ì†Œ)

# Encoderì—ê²Œ ë” ììœ ë¡­ê²Œ í‘œí˜„í•˜ë„ë¡ í—ˆìš©
```

### 3. **Encoder Diversity ëª¨ë‹ˆí„°ë§**
```python
# constants.py
DEBUG_ENCODER = True  # ê³„ì† ì¼œë‘ê¸°

# ì¶œë ¥ í•´ì„:
# Avg similarity > 0.9: ì¸ì½”ë” ë¬¸ì œ, architecture ìˆ˜ì • í•„ìš”
# Avg similarity < 0.7: ì •ìƒ, quantizer ë¬¸ì œë§Œ í•´ê²°í•˜ë©´ ë¨
```

### 4. **ì£¼ê¸°ì  Reset ê³ ë ¤**
í˜„ì¬ëŠ” ë§¤ iterationë§ˆë‹¤ resetí•˜ì§€ë§Œ, ë” ì•ˆì •ì ìœ¼ë¡œ í•˜ë ¤ë©´:
```python
# norm_ema_quantizer.pyì— ì¶”ê°€ ê°€ëŠ¥
if iter_num % reset_interval == 0:
    self.reset_dead_codes(...)
```

---

## ğŸ”¬ ì‹¤í—˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: Baseline í™•ì¸
- [ ] Dead code reset ì—†ì´ í•™ìŠµ (í˜„ì¬ ìƒíƒœ)
- [ ] Codebook ì‚¬ìš©ë¥  ê¸°ë¡ (epochë³„)
- [ ] Encoder diversity ê¸°ë¡

### Phase 2: Dead Code Reset ì ìš©
- [ ] `--dead_code_threshold 1.0`ìœ¼ë¡œ í•™ìŠµ
- [ ] Reset ë¹ˆë„ ë° ê°œìˆ˜ ëª¨ë‹ˆí„°ë§
- [ ] Codebook ì‚¬ìš©ë¥  ë¹„êµ

### Phase 3: Hyperparameter íŠœë‹
- [ ] DECAY: 0.9 â†’ 0.75 ë˜ëŠ” 0.7
- [ ] BETA: 0.25 â†’ 0.1
- [ ] Threshold: 0.5, 1.0, 5.0 ì‹¤í—˜

### Phase 4: ì„±ëŠ¥ í‰ê°€
- [ ] Reconstruction loss í™•ì¸
- [ ] Downstream task ì„±ëŠ¥ (ìˆë‹¤ë©´)
- [ ] Codebook ì‚¬ìš©ë¥  ì•ˆì •ì„±

---

## ğŸ¯ ì„±ê³µ ê¸°ì¤€

### Minimal Success
- Codebook ì‚¬ìš©ë¥  > 30% (epoch 1 ë)
- Validation ì‚¬ìš©ë¥  > 25%

### Target Success
- Codebook ì‚¬ìš©ë¥  > 50% (epoch 1 ë)
- Validation ì‚¬ìš©ë¥  > 45%
- Train-val gap < 10%

### Optimal Success
- Codebook ì‚¬ìš©ë¥  > 70%
- Validation ì‚¬ìš©ë¥  > 65%
- Top code ì‚¬ìš© ë¹ˆë„ < 2x average

---

## ğŸ› Troubleshooting

### ë¬¸ì œ: Resetì´ ë„ˆë¬´ ìì£¼ ë°œìƒ
```
ğŸ”„ Reset 400+ dead codes (ë§¤ iteration)
```
**í•´ê²°**: Thresholdë¥¼ ë‚®ì¶”ê¸° (1.0 â†’ 0.5 ë˜ëŠ” 0.0)

### ë¬¸ì œ: Resetì´ ì „í˜€ ë°œìƒí•˜ì§€ ì•ŠìŒ
```
No dead codes found
```
**ì›ì¸**: Thresholdê°€ ë„ˆë¬´ ë‚®ìŒ (0.0)  
**í•´ê²°**: Threshold ì˜¬ë¦¬ê¸° (0.0 â†’ 1.0 ë˜ëŠ” 5.0)

### ë¬¸ì œ: í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§
```
Lossê°€ íŠ€ê±°ë‚˜ NaN ë°œìƒ
```
**í•´ê²°**: 
1. Threshold ë‚®ì¶”ê¸° (ê³µê²©ì„± ì¤„ì´ê¸°)
2. Reset ì£¼ê¸° ëŠ˜ë¦¬ê¸° (ë§¤ iteration â†’ ë§¤ 10 iterations)
3. Learning rate ë‚®ì¶”ê¸°

### ë¬¸ì œ: ì—¬ì „íˆ collapse ë°œìƒ
```
ì‚¬ìš©ë¥ ì´ ê³„ì† ë–¨ì–´ì§
```
**ì§„ë‹¨**:
1. Encoder diversity í™•ì¸ (avg_similarity > 0.9?)
2. DECAY ë„ˆë¬´ ë†’ì€ì§€ í™•ì¸ (0.9 â†’ 0.7ë¡œ ë‚®ì¶”ê¸°)
3. Dead code reset ë¡œê·¸ í™•ì¸ (ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ”ì§€)

---

## ğŸ“š ì°¸ê³  ìë£Œ

### Dead Code Resetì˜ ì´ë¡ ì  ë°°ê²½
1. **Vector Quantization Literature**
   - "Neural Discrete Representation Learning" (VQ-VAE ì›ë…¼ë¬¸)
   - Codebook collapseëŠ” well-known problem

2. **ë¹„ìŠ·í•œ ê¸°ë²•ë“¤**
   - **K-means restart**: Dead centroidë¥¼ ëœë¤ ìƒ˜í”Œë¡œ ì¬ì´ˆê¸°í™”
   - **EMA with momentum reset**: Momentum ì£¼ê¸°ì  ì´ˆê¸°í™”
   - **Gumbel-Softmax annealing**: Temperature scheduling

3. **ìš°ë¦¬ êµ¬í˜„ì˜ íŠ¹ì§•**
   - EMA ê¸°ë°˜ VQì— ì ìš© (learnable VQì™€ ë‹¤ë¦„)
   - ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ì½”ë“œì˜ ìƒ˜í”Œ ì¬í™œìš© (diversity í–¥ìƒ)
   - Threshold ê¸°ë°˜ adaptive reset (í•™ìŠµ ì§„í–‰ì— ë”°ë¼ ìë™ ì¡°ì •)

---

## ë§ˆë¬´ë¦¬

Dead code resetì€ **EMA ê¸°ë°˜ VQ-VAEì˜ codebook collapseë¥¼ ë°©ì§€í•˜ëŠ” ê°•ë ¥í•œ ê¸°ë²•**ì…ë‹ˆë‹¤.

- âœ… êµ¬í˜„ ê°„ë‹¨ (50ì¤„ ë‚´ì™¸)
- âœ… í•™ìŠµì— í° ì˜í–¥ ì—†ìŒ (ì•ˆì •ì )
- âœ… Hyperparameter íŠœë‹ ì—¬ì§€ ë§ìŒ
- âœ… ì´ë¡ ì  ë°°ê²½ íƒ„íƒ„

í˜„ì¬ ìƒí™© (11.7% validation ì‚¬ìš©ë¥ )ì—ì„œëŠ” **í•„ìˆ˜ì ì¸ ê¸°ë²•**ì´ë©°, DECAY/BETA ì¡°ì •ê³¼ ë³‘í–‰í•˜ë©´ ì‹œë„ˆì§€ íš¨ê³¼ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì§€ê¸ˆ ë°”ë¡œ ì‹¤í—˜í•´ë³´ì„¸ìš”!** ğŸš€
