from utils import load_vq_model, get_token_string
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
import os
import re
import glob

def get_label_from_filename(filename):
    """Extract raw label character from filename (b/cc/m/s)"""
    parts = filename.split('-')
    if len(parts) > 1:      # is_augmented == True
        name = parts[1]
    else:
        name = parts[0]
    return name.split('_')[1]

def parse_label_from_response(response):
    """
    Parse NORMAL/ABNORMAL label from LLM response.
    
    Args:
        response: Full LLM response string
    
    Returns:
        'NORMAL' or 'ABNORMAL' or 'UNKNOWN' if parsing fails
    
    Examples:
        "Result: NORMAL" -> 'NORMAL'
        "Result: ABNORMAL" -> 'ABNORMAL'
        "ë¹„ì •ìƒ ì‹ í˜¸ì…ë‹ˆë‹¤. Result: ABNORMAL" -> 'ABNORMAL'
    """
    
    # Strategy 1: Look for "Result: NORMAL" or "Result: ABNORMAL"
    match = re.search(r'Result:\s*(NORMAL|ABNORMAL)', response, re.IGNORECASE)
    if match:
        return match.group(1).upper()
    
    # Strategy 2: Look for standalone NORMAL/ABNORMAL (case-insensitive)
    # But prioritize if it appears at the end
    lines = response.strip().split('\n')
    for line in reversed(lines):  # Check from bottom up
        if re.search(r'\b(NORMAL|ABNORMAL)\b', line, re.IGNORECASE):
            match = re.search(r'\b(NORMAL|ABNORMAL)\b', line, re.IGNORECASE)
            return match.group(1).upper()
    
    # Strategy 3: Check entire response as fallback
    if re.search(r'\bNORMAL\b', response, re.IGNORECASE):
        return 'NORMAL'
    if re.search(r'\bABNORMAL\b', response, re.IGNORECASE):
        return 'ABNORMAL'
    
    # Failed to parse
    return 'UNKNOWN'

def add_vq_tokens_to_tokenizer(tokenizer, vocab_size=1024):
    """
    Add VQ token vocabulary (<TOK_0> ~ <TOK_1024>) to tokenizer.json
    
    Args:
        tokenizer: HuggingFace tokenizer object
        vocab_size: Number of VQ codebook tokens (default: 1024)
    """
    
    tokenizer_path = tokenizer.vocab_files_names.get('tokenizer_file')
    if not tokenizer_path:
        # Try to find tokenizer.json in the model directory
        model_name = tokenizer.name_or_path
        potential_paths = [
            os.path.join(model_name, 'tokenizer.json'),
            'tokenizer.json'
        ]
        for path in potential_paths:
            if os.path.exists(path):
                tokenizer_path = path
                break
    
    if not tokenizer_path or not os.path.exists(tokenizer_path):
        print(f"âŒ tokenizer.json not found")
        return False
    
    # Load tokenizer.json
    with open(tokenizer_path, 'r', encoding='utf-8') as f:
        tokenizer_json = json.load(f)
    
    # Get current vocab
    current_vocab = tokenizer_json.get('model', {}).get('vocab', {})
    
    # Add VQ tokens
    for i in range(vocab_size):
        tok_str = f"<TOK_{i}>"
        if tok_str not in current_vocab:
            current_vocab[tok_str] = len(current_vocab)
    
    # Save updated tokenizer.json
    with open(tokenizer_path, 'w', encoding='utf-8') as f:
        json.dump(tokenizer_json, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… Added {vocab_size} VQ tokens to tokenizer.json")
    return True

def create_prompt(token_string, normal_dist_info=None, abnormal_dist_info=None):
    """
    Create prompt for anomaly detection based on token distribution characteristics.
    
    ë¶„í¬ì˜ "ì§‘ì¤‘ë„"ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŒì •:
    - NORMAL: í•œë‘ ê°œ í† í°ì´ ì§€ë°°ì  (ë¾°ì¡±í•œ ë¶„í¬)
    - ABNORMAL: ì—¬ëŸ¬ í† í°ì´ ê· ë“± ë¶„ì‚° (í‰íƒ„í•œ ë¶„í¬)
    
    Args:
        token_string: Token string from VQ model (e.g., "<TOK_776> <TOK_687> ...")
        normal_dist_info: Dict with keys like 'top_10_ratio', 'top_50_ratio', 'entropy'
        abnormal_dist_info: Same format as normal_dist_info
    
    Returns:
        Prompt string for LLM
    """
    
    # ê¸°ë³¸ê°’ ì„¤ì • (analyze_tokens_unified.py ê²°ê³¼ ê¸°ë°˜)
    if normal_dist_info is None:
        normal_dist_info = {
            'top_1_token': 'Token 776',
            'top_1_ratio': 7.19,
            'top_10_ratio': 45.0,  # ìƒìœ„ 10ê°œ ëˆ„ì  ë¹„ìœ¨ (ì•½)
            'top_50_ratio': 90.0,  # ìƒìœ„ 50ê°œ ëˆ„ì  ë¹„ìœ¨ (ì•½)
            'description': 'í•œë‘ ê°œ í† í°ì´ ê°•í•˜ê²Œ ì§€ë°°ì  (ë¾°ì¡±í•œ ë¶„í¬)',
        }
    
    if abnormal_dist_info is None:
        abnormal_dist_info = {
            'top_1_token': 'Token 110',
            'top_1_ratio': 2.18,
            'top_10_ratio': 20.0,  # ìƒìœ„ 10ê°œ ëˆ„ì  ë¹„ìœ¨ (ì•½)
            'top_50_ratio': 50.0,  # ìƒìœ„ 50ê°œ ëˆ„ì  ë¹„ìœ¨ (ì•½)
            'description': 'ë§ì€ í† í°ë“¤ì´ ê±°ì˜ ë™ë“±í•˜ê²Œ ë¶„ì‚° (í‰íƒ„í•œ ë¶„í¬)',
        }
    
    prompt = f"""ì‹ í˜¸ ì´ìƒíƒì§€ - í† í° ë¶„í¬ ê¸°ë°˜ ë¶„ì„ (Signal Anomaly Detection)

## ğŸ“Š ë¶„ì„ ì›ë¦¬:

ì •ìƒê³¼ ë¹„ì •ìƒ ì‹ í˜¸ëŠ” **í† í° ë¶„í¬ì˜ í˜•íƒœ**ê°€ ë‹¤ë¦…ë‹ˆë‹¤:

### NORMAL ì‹ í˜¸ì˜ íŠ¹ì§•:
- **ë¶„í¬ í˜•íƒœ**: ë¾°ì¡±í•¨ (Sharp distribution)
- **ìµœìƒìœ„ í† í°**: {normal_dist_info['top_1_token']} ({normal_dist_info['top_1_ratio']:.2f}% ì ìœ )
- **ìƒìœ„ 10ê°œ ëˆ„ì **: ~{normal_dist_info['top_10_ratio']:.0f}% (ë†’ì€ ì§‘ì¤‘ë„)
- **ìƒìœ„ 50ê°œ ëˆ„ì **: ~{normal_dist_info['top_50_ratio']:.0f}% 
- **ì˜ë¯¸**: {normal_dist_info['description']}

### ABNORMAL ì‹ í˜¸ì˜ íŠ¹ì§•:
- **ë¶„í¬ í˜•íƒœ**: í‰íƒ„í•¨ (Flat distribution)
- **ìµœìƒìœ„ í† í°**: {abnormal_dist_info['top_1_token']} ({abnormal_dist_info['top_1_ratio']:.2f}% ì ìœ )
- **ìƒìœ„ 10ê°œ ëˆ„ì **: ~{abnormal_dist_info['top_10_ratio']:.0f}% (ë‚®ì€ ì§‘ì¤‘ë„)
- **ìƒìœ„ 50ê°œ ëˆ„ì **: ~{abnormal_dist_info['top_50_ratio']:.0f}%
- **ì˜ë¯¸**: {abnormal_dist_info['description']}

---

## ğŸ” íŒì • ê¸°ì¤€:

ì…ë ¥ ì‹ í˜¸ì˜ í† í° ë¶„í¬ë¥¼ ë¶„ì„í•˜ì—¬:

1. **ë¶„í¬ ì§‘ì¤‘ë„ í™•ì¸**
   - ìƒìœ„ 5ê°œ í† í°ì´ ì „ì²´ì˜ ëª‡ %ë¥¼ ì°¨ì§€í•˜ëŠ”ê°€?
   - ìƒìœ„ 10ê°œ í† í°ì´ ì „ì²´ì˜ ëª‡ %ë¥¼ ì°¨ì§€í•˜ëŠ”ê°€?
   - NORMAL: 30~50% (ì§‘ì¤‘)
   - ABNORMAL: 10~20% (ë¶„ì‚°)

2. **ìµœìƒìœ„ í† í° ë¶„ì„**
   - ìµœìƒìœ„ í† í°ì´ ì–¼ë§ˆë‚˜ ì§€ë°°ì ì¸ê°€?
   - NORMAL: ìµœìƒìœ„ í† í°ì´ 5% ì´ìƒ (ë¾°ì¡±í•¨)
   - ABNORMAL: ìµœìƒìœ„ í† í°ì´ 2~3% ì •ë„ (í‰íƒ„í•¨)

3. **ë¶„í¬ì˜ ë‹¤ì–‘ì„±**
   - ì‚¬ìš©ë˜ëŠ” ê³ ìœ  í† í°ì˜ ìˆ˜ê°€ ë§ì€ê°€?
   - ì—¬ëŸ¬ í† í°ì´ ë¹„ìŠ·í•œ ë¹ˆë„ë¡œ ë‚˜íƒ€ë‚˜ëŠ”ê°€?
   - NORMAL: í† í°ì´ í•œì •ì , ì¼ë¶€ ì§€ë°°ì 
   - ABNORMAL: í† í°ì´ ë‹¤ì–‘í•¨, ê· ë“± ë¶„ì‚°

---

## ë¶„ì„ ëŒ€ìƒ ì‹ í˜¸ (200 í† í°):

{token_string}

---

## ğŸ“‹ ë¶„ì„ ì‘ì—…:

ìœ„ ì‹ í˜¸ì˜ í† í° ë¶„í¬ë¥¼ ê³„ì‚°í•˜ì—¬:
1. ìƒìœ„ 5ê°œ, 10ê°œ, 20ê°œ í† í°ì˜ ëˆ„ì  ë¹„ìœ¨ ê³„ì‚°
2. ìµœìƒìœ„ í† í°ì´ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨
3. ë¶„í¬ì˜ ì§‘ì¤‘ë„ (ë¾°ì¡±í•œê°€? í‰íƒ„í•œê°€?)
4. NORMAL ë¶„í¬ì™€ ABNORMAL ë¶„í¬ ì¤‘ ì–´ëŠ ìª½ì— ë” ê°€ê¹Œìš´ê°€?

## ğŸ¯ ìµœì¢… íŒì •:

ì§‘ì¤‘ë„ê°€ ë†’ìœ¼ë©´ (ë¾°ì¡±í•˜ë©´) â†’ **Result: NORMAL**
ì§‘ì¤‘ë„ê°€ ë‚®ìœ¼ë©´ (í‰íƒ„í•˜ë©´) â†’ **Result: ABNORMAL**

## ì¶œë ¥ ì˜ˆì‹œ:

ì •ìƒ ì‹ í˜¸ ì˜ˆ:
ìƒìœ„ 5ê°œ ëˆ„ì  ë¹„ìœ¨: 35%
ìµœìƒìœ„ í† í°: 8%
ë¶„í¬: ë¾°ì¡±í•œ í˜•íƒœ
Result: NORMAL

ë¹„ì •ìƒ ì‹ í˜¸ ì˜ˆ:
ìƒìœ„ 5ê°œ ëˆ„ì  ë¹„ìœ¨: 12%
ìµœìƒìœ„ í† í°: 2.5%
ë¶„í¬: í‰íƒ„í•œ í˜•íƒœ
Result: ABNORMAL
"""
    
    return prompt



# ===== Configuration =====
VQ_CHECKPOINT = "C:\\Users\\myqkr\\Desktop\\SignalLM\\ckpt-19.pt"
DATA_DIR = "C:\\Users\\myqkr\\Desktop\\SignalLM\\pkl_data\\test"
LLM_MODEL = "Qwen/Qwen-0.6B"
DEVICE = "cpu"

# ===== Main Execution =====
if __name__ == "__main__":
    # 1. Load VQ model
    print("ğŸ”„ Loading VQ model...")
    vq_model = load_vq_model(VQ_CHECKPOINT, device=DEVICE, weights_only=False)
    print(f"âœ… VQ model loaded from {VQ_CHECKPOINT}\n")
    
    # 2. Get all pkl files
    files = glob.glob(os.path.join(DATA_DIR, "*.pkl"))
    print(f"ğŸ“‚ Found {len(files)} files in {DATA_DIR}\n")
    
    if len(files) == 0:
        print("âŒ No pkl files found!")
        exit(1)
    
    # 3. Load LLM model
    print("ğŸ”„ Loading LLM model...")
    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)
    llm_model = AutoModelForCausalLM.from_pretrained(LLM_MODEL)
    print(f"âœ… LLM model loaded: {LLM_MODEL}\n")
    
    # 4. Statistics
    correct = 0
    total = 0
    results = []
    
    # 5. Process each file
    for idx, filename in enumerate(files, 1):
        print("="*80)
        print(f"ğŸ“„ Processing [{idx}/{len(files)}]: {os.path.basename(filename)}")
        print("="*80)
        
        # Extract tokens
        token_string = get_token_string(vq_model, filename, identifier="TOK")
        label = get_label_from_filename(os.path.basename(filename))
        
        # Create conversation
        conversation = [
            # Round 1: íƒœìŠ¤í¬ ì„¤ëª…
            {
                "role": "user",
                "content": "ì•„ë‚ ë¡œê·¸ ì „ìê¸° ì‹ í˜¸ë¥¼ VQ-VAEë¥¼ ì‚¬ìš©í•´ì„œ í† í°í™”ë¥¼ í–ˆë‹¤. ì „ì²´ 20ì´ˆ ì§œë¦¬ ì‹ í˜¸ë¥¼ 0.1ì´ˆ ë‹¨ìœ„ë¡œ ë‚˜ëˆ ì„œ, í•˜ë‚˜ì˜ í† í°ì´ ë˜ë„ë¡ í•˜ì—¬ ì´ 200ê°œì˜ í† í° ì‹œí€€ìŠ¤ê°€ ìˆë‹¤. ì´ í† í° ì‹œí€€ìŠ¤ë¥¼ ë¶„ì„í•´ì„œ ì‹ í˜¸ì˜ ë ˆì´ë¸”-ì •ìƒ í˜¹ì€ ë¹„ì •ìƒ-ì„ ì œë¡œìƒ·ìœ¼ë¡œ íƒì§€í•´ì•¼ í•˜ëŠ”ë°, ê·¸ ë°©ë²•ì„ ì´ì œë¶€í„° ì•Œë ¤ì¤„ê²Œ."
            },
            {
                "role": "assistant",
                "content": "ì‘. ì‹ í˜¸ í† í° ë¶„ì„í•´ì„œ ì •ìƒ í˜¹ì€ ë¹„ì •ìƒìœ¼ë¡œ ë¶„ë¥˜í•˜ê² ìŠµë‹ˆë‹¤."
            },
            
            # Round 2: íŒì • ê¸°ì¤€ ì„¤ëª…
            {
                "role": "user",
                "content": "ì •ìƒì‹ í˜¸ëŠ” <TOK_257>, <TOK_390>, ...ê³¼ ê°™ì€ í† í°ì´ ìì£¼ ë‚˜íƒ€ë‚˜. ì´ê²Œ ìì£¼ ë“±ì¥í•˜ëŠ” í† í°ì¸ë°, ì´ í† í°ì€ ì •ìƒ ì‹ í˜¸ì™€ ë¹„ì •ìƒ ì‹ í˜¸ ëª¨ë‘ì—ì„œ ê³µí†µë˜ê²Œ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” í† í°ì´ì•¼. ê·¸ëŸ°ë° ë°˜ëŒ€ë¡œ, ìì£¼ ë“±ì¥í•˜ì§€ ì•ŠëŠ” í† í°ì€ ì •ìƒì‹ í˜¸ì—ì„œë§Œ ë‚˜íƒ€ë‚˜. ë‹¤ì‹œë§í•´ì„œ, ì •ìƒì‹ í˜¸ì—ì„œ ì¶œí˜„ë¹ˆë„ê°€ ë‚®ì€ í† í°ë“¤ì´ ë“±ì¥í•œë‹¤ë©´ ê·¸ í† í° ì‹œí€€ìŠ¤ëŠ” ì •ìƒ ì‹ í˜¸ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§€ê³ , ê·¸ í† í°ë“¤ì´ ë“±ì¥í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ë¹„ì •ìƒ ì‹ í˜¸ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§€ëŠ”ê±°ì•¼."
            },
            {
                "role": "assistant",
                "content": "ì‘ ê³ ë§ˆì›Œ. ê·¸ë ‡ë‹¤ë©´ ì´ì œ ì‹ í˜¸ë¥¼ ë¶„ì„í•´ë³¼ê¹Œ?"
            },
            
            # Round 3: ì‹¤ì œ ë¶„ì„ ìš”ì²­
            {
                "role": "user",
                "content": f"ì‘, ì´ì œ í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë‹¬í• ê²Œ.\n\n{create_prompt(token_string)}"
            }
        ]
        
        # Generate prompt with chat template
        prompt = tokenizer.apply_chat_template(
            conversation, 
            tokenize=False,
            add_generation_prompt=True
        )
        
        # Inference
        inputs = tokenizer(prompt, return_tensors="pt")
        outputs = llm_model.generate(**inputs, max_new_tokens=200, do_sample=True, temperature=0.7)
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Parse label
        predicted_label = parse_label_from_response(response)
        
        # Check correctness
        is_correct = (predicted_label == label.upper())
        if is_correct:
            correct += 1
        total += 1
        
        # Store result
        results.append({
            "filename": os.path.basename(filename),
            "true_label": label,
            "predicted_label": predicted_label,
            "correct": is_correct
        })
        
        # Print result
        print(f"ğŸ¤– Predicted: {predicted_label}")
        print(f"âœ… True Label: {label}")
        print(f"{'âœ… CORRECT!' if is_correct else 'âŒ WRONG'}")
        print()
    
    # 6. Print summary
    print("="*80)
    print("ğŸ“Š FINAL RESULTS")
    print("="*80)
    print(f"Total Files: {total}")
    print(f"Correct: {correct}")
    print(f"Wrong: {total - correct}")
    print(f"Accuracy: {correct/total*100:.2f}%")
    print("="*80)
    
    # 7. Print detailed results
    print("\nğŸ“‹ Detailed Results:")
    print("-"*80)
    for result in results:
        status = "âœ…" if result["correct"] else "âŒ"
        print(f"{status} {result['filename']}: {result['true_label']} -> {result['predicted_label']}")
    print("="*80)

    from utils import load_vq_model, get_token_string
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
import os



def add_vq_tokens_to_tokenizer(tokenizer, vocab_size=1024):
    """
    Add VQ token vocabulary (<TOK_0> ~ <TOK_1024>) to tokenizer.json
    
    Args:
        tokenizer: HuggingFace tokenizer object
        vocab_size: Number of VQ codebook tokens (default: 1024)
    """
    
    tokenizer_path = tokenizer.vocab_files_names.get('tokenizer_file')
    if not tokenizer_path:
        # Try to find tokenizer.json in the model directory
        model_name = tokenizer.name_or_path
        potential_paths = [
            os.path.join(model_name, 'tokenizer.json'),
            'tokenizer.json'
        ]
        for path in potential_paths:
            if os.path.exists(path):
                tokenizer_path = path
                break
    
    if not tokenizer_path or not os.path.exists(tokenizer_path):
        print(f"âŒ tokenizer.json not found")
        return False
    
    # Load tokenizer.json
    with open(tokenizer_path, 'r', encoding='utf-8') as f:
        tokenizer_json = json.load(f)
    
    # Get current vocab
    current_vocab = tokenizer_json.get('model', {}).get('vocab', {})
    
    # Add VQ tokens
    for i in range(vocab_size):
        tok_str = f"<TOK_{i}>"
        if tok_str not in current_vocab:
            current_vocab[tok_str] = len(current_vocab)
    
    # Save updated tokenizer.json
    with open(tokenizer_path, 'w', encoding='utf-8') as f:
        json.dump(tokenizer_json, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… Added {vocab_size} VQ tokens to tokenizer.json")
    return True

def create_prompt(token_string, label=None, normal_tokens_set=None):
    """
    Create a prompt for LLM to analyze VQ-VAE tokens using token SET logic.
    
    ê¸°ë³¸ ì›ë¦¬: ì •ìƒ ì‹ í˜¸ì—ì„œëŠ” íŠ¹ì • í† í° ì„¸íŠ¸ê°€ ìì£¼ ë‚˜íƒ€ë‚˜ê³ ,
    ë¹„ì •ìƒ ì‹ í˜¸ì—ì„œëŠ” ì´ ì„¸íŠ¸ì— ì—†ëŠ” ë‹¤ë¥¸ í† í°ë“¤ì´ ìì£¼ ë‚˜íƒ€ë‚¨.
    -> "ì •ìƒ ë¶„í¬ì— ì—†ëŠ” í† í°ì´ ë³´ì´ë©´ ë¹„ì •ìƒìœ¼ë¡œ íŒë³„"
    
    Args:
        token_string: Space-separated string of tokens (e.g., "<TOK_703> <TOK_266> ...")
        label: Optional label for debugging
        normal_tokens_set: Set of token IDs that appear in normal signals (default: ckpt-29 top 20)
    
    Returns:
        Prompt string for LLM
    """
    
    # Default normal token set from ckpt-29 analysis (top 20 tokens)
    # These are the tokens most frequently appearing in normal signals
    if normal_tokens_set is None:
        normal_tokens_set = {776, 687, 254, 1, 605, 582, 121, 789, 26, 117, 
                            207, 195, 58, 110, 535, 280, 47, 670, 819, 458}
    
    # Convert token IDs to token strings for readability in prompt
    normal_tokens_str = ", ".join([f"<TOK_{tok}>" for tok in sorted(normal_tokens_set)])
    
    prompt = f"""ì‹ í˜¸ ì´ìƒíƒì§€ (Signal Anomaly Detection) - Token SET ê¸°ë°˜ ë¶„ì„

## ì •ìƒ(Normal) ì‹ í˜¸ì˜ íŠ¹ì§•:

### ì •ìƒ ì‹ í˜¸ì—ì„œ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” í•µì‹¬ í† í° ì„¸íŠ¸ (Token SET):
{normal_tokens_str}

**í•µì‹¬ ì›ë¦¬**: 
- ì •ìƒ ì‹ í˜¸ëŠ” ìœ„ì˜ í† í°ë“¤ë¡œ ì£¼ë¡œ êµ¬ì„±ë¨ (33.5% ì½”ë“œë¶ ì‚¬ìš©ëŸ‰)
- ë¹„ì •ìƒ ì‹ í˜¸ëŠ” ì´ ì„¸íŠ¸ì— ì—†ëŠ” ë‹¤ë¥¸ í† í°ë“¤ì´ ë§ì´ í¬í•¨ë¨
- ì •ìƒ ë¶„í¬ì— ì—†ëŠ” í† í°ì´ ë³´ì´ë©´ ë¹„ì •ìƒìœ¼ë¡œ íŒë³„ ê°€ëŠ¥

---

## íŒì • ê¸°ì¤€ (Token SET ê¸°ë°˜):

1. **ì…ë ¥ ì‹ í˜¸ì˜ í† í° ë¶„ì„**
   - ì‹ í˜¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜
   - ë‚˜íƒ€ë‚˜ëŠ” ëª¨ë“  í† í° ëª©ë¡ ì¶”ì¶œ
   - ê° í† í°ì˜ ë“±ì¥ ë¹ˆë„ ê³„ì‚°

2. **ì •ìƒ í† í° ì„¸íŠ¸ì™€ ë¹„êµ**
   - ì…ë ¥ ì‹ í˜¸ì˜ í† í°ë“¤ì´ ì •ìƒ ì„¸íŠ¸ì— ì–¼ë§ˆë‚˜ í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
   - ì •ìƒ ì„¸íŠ¸ì— **ì—†ëŠ”** ìƒˆë¡œìš´ í† í°ë“¤ ì‹ë³„
   - ìƒˆë¡œìš´ í† í°ë“¤ì˜ ë“±ì¥ ë¹ˆë„ í™•ì¸

3. **ìµœì¢… íŒì •**
   - ì…ë ¥ ì‹ í˜¸ì˜ ëŒ€ë¶€ë¶„ í† í°ì´ ì •ìƒ ì„¸íŠ¸ì— í¬í•¨ë¨ â†’ **NORMAL**
   - ì…ë ¥ ì‹ í˜¸ì—ì„œ ì •ìƒ ì„¸íŠ¸ì— ì—†ëŠ” ìƒˆë¡œìš´ í† í°ë“¤ì´ ë§ì´/ìì£¼ ë‚˜íƒ€ë‚¨ â†’ **ABNORMAL**

---

## ë¶„ì„ ëŒ€ìƒ ì‹ í˜¸:

{token_string}

ìœ„ ì‹ í˜¸ë¥¼ ë¶„ì„í•˜ì—¬:

1. ì‹ í˜¸ì— ë‚˜íƒ€ë‚˜ëŠ” ëª¨ë“  í† í° ì¶”ì¶œ
2. ì •ìƒ í† í° ì„¸íŠ¸ í™•ì¸: {{{normal_tokens_str}}}
3. ì…ë ¥ ì‹ í˜¸ì˜ í† í° ì¤‘ ì •ìƒ ì„¸íŠ¸ í¬í•¨ë„ ê³„ì‚° (%)
4. ì •ìƒ ì„¸íŠ¸ì— ì—†ëŠ” ì´ìƒ í† í° ì‹ë³„
5. "ì •ìƒ ë¶„í¬ì— ì—†ëŠ” í† í°ì´ ë³´ì´ë©´ ë¹„ì •ìƒìœ¼ë¡œ íŒë³„" ì›ì¹™ ì ìš©

**ìµœì¢… íŒì •: [NORMAL / ABNORMAL]**
**ì‹ ë¢°ë„: [ë†’ìŒ / ì¤‘ê°„ / ë‚®ìŒ]**
**ì •ìƒ ì„¸íŠ¸ í¬í•¨ë„: [%]**
**ì´ìƒ í† í° ì‹ë³„: [ìƒˆë¡œìš´ í† í°ë“¤]**
**íŒì • ê·¼ê±°: [ì •ìƒ í† í° ì„¸íŠ¸ í¬í•¨ë„ ë° ì´ìƒ í† í° ë¹ˆë„ ë¶„ì„]**"""
    
    return prompt


# ============================================================================
# ì‚¬ìš© ì˜ˆì‹œ
# ============================================================================

if __name__ == "__main__":
    # 1. VQ ëª¨ë¸ë¡œ ì‹ í˜¸ë¥¼ í† í°í™”
    vq_model = load_vq_model("./vq_output/checkpoints/VQ/ckpt_29.pt")
    token_string = get_token_string(vq_model, "signal.csv", identifier="TOK")
    # ê²°ê³¼: "<TOK_703> <TOK_266> <TOK_536> ..."

    # 2. LLM ë¡œë“œ (Qwen 0.6B)
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-0.6B")
    llm_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-0.6B")

    # 3. VQ í† í°ì„ tokenizerì— ì¶”ê°€
    add_vq_tokens_to_tokenizer(tokenizer)

    # 4. í”„ë¡¬í”„íŠ¸ ìƒì„± (ìƒˆë¡œìš´ token SET ê¸°ë°˜ ë¡œì§)
    # ê¸°ë³¸ê°’ ì‚¬ìš© (ckpt-29 ë¶„ì„ ê²°ê³¼)
    prompt = create_prompt(token_string)
    
    # ë˜ëŠ” ì»¤ìŠ¤í…€ normal_tokens_set ì‚¬ìš© ê°€ëŠ¥:
    # custom_normal_set = {776, 687, 254, 1, 605, 582, 121, 789, ...}
    # prompt = create_prompt(token_string, normal_tokens_set=custom_normal_set)

    # 5. ì¶”ë¡ 
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = llm_model.generate(**inputs, max_new_tokens=150)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print(response)

'''
ìƒê°ê±°ë¦¬


ì. ìš°ë¦¬ê°€ ì´ì œ ì•Œê³ ìˆëŠ”ê±´ ì•„ë˜ì™€ ê°™ì•„.

NORMAL ë¶„í¬ì—ì„œ ì—„ì²­ ë§ì´ ì“°ì´ê³ , ABNORMALì—ì„œëŠ” ê±°ì˜ ì•ˆ ì“°ì´ëŠ” í† í°ë“¤
ë°˜ëŒ€ë¡œ, ABNORMAL ë¶„í¬ì—ì„œ ë§ì´ ì“°ì´ê³ , NORMALì—ì„œëŠ” ê±°ì˜ ì•ˆ ì“°ì´ëŠ” í† í°ë“¤
í•˜ì§€ë§Œ, ë‚˜ëŠ” 200ê°œì˜ í† í°ì‹œí€€ìŠ¤ë¥¼ ì „ë‹¬í•˜ë©° í•´ë‹¹ ì‹œí€€ìŠ¤ëŠ” "ì©¡ìƒ"ì¸ì§€ "ë¹„ì •ìƒ"ì¸ì§€ ë¬¼ì„ ì˜ˆì •.
ë”°ë¼ì„œ,
NORMAL ë¶„í¬ì—ì„œ "ì—„ì²­ ë§ì´ ì“°ì´ëŠ”" í† í°ì´ ì‹¤ì œ 200ê°œ í† í°ì‹œí€€ìŠ¤ì—ì„œ "ì—„ì²­ ë§ì´" ë‚˜ì˜¤ì§„ ì•Šì•„. ë˜, ê·¸ë ‡ë‹¤ê³  normal í† í°ì´ abnormalì—ì„œ ê±°ì˜ ì•ˆì“°ì´ëŠ”ê±´ ë˜ ì•„ë‹Œê²½ìš°ë„ ìˆì–´. "ì ê²Œ" ì“°ì¼ ë¿.

ë§ì•„, ê·¸ í¬ì¸íŠ¸ê°€ í•µì‹¬ì´ì•¼.

200 í† í°ì´ë©´ ê°œë³„ í† í°ì´ â€œì—„ì²­ ë§ì´â€ ë‚˜ì˜¬ ìˆ˜ê°€ ì—†ê³ 

NORMALì—ì„œ ë§ì´ ì“°ì´ëŠ” í† í°ë„ ABNORMALì—ì„œ 0ì´ ì•„ë‹ˆë¼ ê·¸ëƒ¥ ë” ì ê²Œ ë‚˜ì˜¬ ë¿ì´ë¼
â€œì´ í† í°ì´ 3% ì´ìƒì´ë©´ NORMALâ€ ê°™ì€ ë£°ì€ í˜„ì‹¤ì„±ì´ ê±°ì˜ ì—†ì–´.

ê·¸ë˜ì„œ â€œì ˆëŒ€ ë¹„ìœ¨â€ì´ ì•„ë‹ˆë¼ â€œë¹„ìœ¨ì˜ ì°¨ì´â€ë¥¼ ì—¬ëŸ¬ í† í°ì— ê±¸ì³ ì¡°ê¸ˆì”© í•©ì‚°í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê°€ì•¼ í•´.
ê·¸ê²Œ ì‚¬ì‹¤ìƒ Naive Bayes / log-odds ëŠë‚Œì´ê³ , ì§§ì€ ì‹œí€€ìŠ¤ì— ê°€ì¥ ì˜ ë§ëŠ” ë°©ë²•ì´ì•¼.

ìœ„ ë‚´ìš©ì„ ë°˜ì˜í•´ì„œ promptë¥¼ ë§Œë“¤ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œ?
prompt ì´ìƒì˜ ë°©ë²•ì´ ìˆì„ê¹Œ?
'''