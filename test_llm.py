from utils import load_vq_model, get_token_string
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
import os
import re
import glob

def get_label_from_filename(filename):
    """Extract raw label character from filename (b/cc/m/s)"""
    parts = filename.split('-')
    if len(parts) > 1:      # is_augmented == True
        name = parts[1]
    else:
        name = parts[0]
    return name.split('_')[1]

def parse_label_from_response(response):
    """
    Parse NORMAL/ABNORMAL label from LLM response.
    
    Args:
        response: Full LLM response string
    
    Returns:
        'NORMAL' or 'ABNORMAL' or 'UNKNOWN' if parsing fails
    
    Examples:
        "Result: NORMAL" -> 'NORMAL'
        "Result: ABNORMAL" -> 'ABNORMAL'
        "ë¹„ì •ìƒ ì‹ í˜¸ì…ë‹ˆë‹¤. Result: ABNORMAL" -> 'ABNORMAL'
    """
    
    # Strategy 1: Look for "Result: NORMAL" or "Result: ABNORMAL"
    match = re.search(r'Result:\s*(NORMAL|ABNORMAL)', response, re.IGNORECASE)
    if match:
        return match.group(1).upper()
    
    # Strategy 2: Look for standalone NORMAL/ABNORMAL (case-insensitive)
    # But prioritize if it appears at the end
    lines = response.strip().split('\n')
    for line in reversed(lines):  # Check from bottom up
        if re.search(r'\b(NORMAL|ABNORMAL)\b', line, re.IGNORECASE):
            match = re.search(r'\b(NORMAL|ABNORMAL)\b', line, re.IGNORECASE)
            return match.group(1).upper()
    
    # Strategy 3: Check entire response as fallback
    if re.search(r'\bNORMAL\b', response, re.IGNORECASE):
        return 'NORMAL'
    if re.search(r'\bABNORMAL\b', response, re.IGNORECASE):
        return 'ABNORMAL'
    
    # Failed to parse
    return 'UNKNOWN'

def add_vq_tokens_to_tokenizer(tokenizer, vocab_size=1024):
    """
    Add VQ token vocabulary (<TOK_0> ~ <TOK_1024>) to tokenizer.json
    
    Args:
        tokenizer: HuggingFace tokenizer object
        vocab_size: Number of VQ codebook tokens (default: 1024)
    """
    
    tokenizer_path = tokenizer.vocab_files_names.get('tokenizer_file')
    if not tokenizer_path:
        # Try to find tokenizer.json in the model directory
        model_name = tokenizer.name_or_path
        potential_paths = [
            os.path.join(model_name, 'tokenizer.json'),
            'tokenizer.json'
        ]
        for path in potential_paths:
            if os.path.exists(path):
                tokenizer_path = path
                break
    
    if not tokenizer_path or not os.path.exists(tokenizer_path):
        print(f"âŒ tokenizer.json not found")
        return False
    
    # Load tokenizer.json
    with open(tokenizer_path, 'r', encoding='utf-8') as f:
        tokenizer_json = json.load(f)
    
    # Get current vocab
    current_vocab = tokenizer_json.get('model', {}).get('vocab', {})
    
    # Add VQ tokens
    for i in range(vocab_size):
        tok_str = f"<TOK_{i}>"
        if tok_str not in current_vocab:
            current_vocab[tok_str] = len(current_vocab)
    
    # Save updated tokenizer.json
    with open(tokenizer_path, 'w', encoding='utf-8') as f:
        json.dump(tokenizer_json, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… Added {vocab_size} VQ tokens to tokenizer.json")
    return True

def create_prompt(token_string):
    """
    Create prompt template for anomaly detection based on token distribution.
    
    Args:
        token_string: Token string from VQ model (e.g., "<TOK_257> <TOK_390> <TOK_912> ...")
    
    Returns:
        Prompt string for LLM
    """
    
    prompt = f"""ì‹ í˜¸ ì´ìƒíƒì§€ (Signal Anomaly Detection)

## ì •ìƒ(Normal) ì‹ í˜¸ì˜ íŠ¹ì§•:

### ë“±ì¥ íšŸìˆ˜ ê¸°ì¤€ ìƒìœ„ 20ê°œ í† í°:
1ìˆœìœ„: <TOK_257> (21.3%)
2ìˆœìœ„: <TOK_390> (19.5%)
3ìˆœìœ„: <TOK_912> (14.6%)
4ìˆœìœ„: <TOK_117> (7.6%)
5ìˆœìœ„: <TOK_947> (6.3%)
6ìˆœìœ„: <TOK_340> (6.0%)
7ìˆœìœ„: <TOK_701> (5.5%)
8ìˆœìœ„: <TOK_727> (4.9%) â­ 8~20ìˆœìœ„ê°€ í•µì‹¬
9ìˆœìœ„: <TOK_63> (3.9%)
10ìˆœìœ„: <TOK_480> (3.4%)
11ìˆœìœ„: <TOK_516> (2.2%)
12ìˆœìœ„: <TOK_138> (1.4%)
13ìˆœìœ„: <TOK_623> (0.9%)
14ìˆœìœ„: <TOK_743> (0.6%)
15ìˆœìœ„: <TOK_787> (0.6%)
16ìˆœìœ„: <TOK_861> (0.5%)
17ìˆœìœ„: <TOK_118> (0.4%)
18ìˆœìœ„: <TOK_681> (0.3%)
19ìˆœìœ„: <TOK_937> (0.3%)
20ìˆœìœ„: <TOK_79> (0.2%)

**ì •ìƒ ì‹ í˜¸ì—ì„œ 8~20ìˆœìœ„(ë“±ì¥ íšŸìˆ˜ ê¸°ì¤€)ë¡œ ë§ì´ ë‚˜íƒ€ë‚˜ëŠ” í† í°ë“¤: <TOK_727>, <TOK_63>, <TOK_480>, <TOK_516>, <TOK_138>, <TOK_623>, <TOK_743>, <TOK_787>, <TOK_861>, <TOK_118>, <TOK_681>, <TOK_937>, <TOK_79>**

---

## ì •ìƒ ë° ë¹„ì •ìƒ íŒì • ê¸°ì¤€:

ì…ë ¥ëœ ì‹ í˜¸ì˜ í† í° ë“±ì¥ ë¹ˆë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ 20ê°œë¥¼ ì¶”ì¶œí•˜ì—¬ ë¶„ì„:

1. **ì •ìƒ ì‹ í˜¸ì˜ 8~20ìˆœìœ„ í† í°ë“¤ í™•ì¸**
   - <TOK_727>, <TOK_63>, <TOK_480>, <TOK_516>, <TOK_138>, <TOK_623>, <TOK_743>, <TOK_787>, <TOK_861>, <TOK_118>, <TOK_681>, <TOK_937>, <TOK_79>

2. **ì…ë ¥ ì‹ í˜¸ì—ì„œë„ ê°™ì€ í† í°ë“¤ì´ ë“±ì¥ ë¹ˆë„ ìƒìœ„ 20ìœ„ ë‚´ì— ë§ì´ í¬í•¨ë˜ëŠ”ì§€ í™•ì¸**
   - ì •ìƒì˜ 8~20ìˆœìœ„ í† í°ë“¤ì´ ì…ë ¥ ì‹ í˜¸ì—ì„œë„ ìƒìœ„ 20ìœ„ ë‚´ì— ë§ì´ ë‚˜íƒ€ë‚¨ â†’ NORMAL
   - ì •ìƒì˜ 8~20ìˆœìœ„ í† í°ë“¤ì´ ì…ë ¥ ì‹ í˜¸ì—ì„œ ìƒìœ„ 20ìœ„ ë°–ìœ¼ë¡œ ë°€ë ¤ë‚¨ â†’ ABNORMAL

---

## ë¶„ì„ ëŒ€ìƒ ì‹ í˜¸:
{token_string}

## ì¶œë ¥ í¬ë§·
Result: [NORMAL / ABNORMAL]

## ì¶œë ¥ ì˜ˆì‹œ
ì •ìƒì¸ ê²½ìš°
Result: NORMAL

ë¹„ì •ìƒì¸ ê²½ìš°
Result: ABNORMAL
"""
    
    return prompt



# ===== Configuration =====
VQ_CHECKPOINT = "C:\\Users\\myqkr\\Desktop\\SignalLM\\ckpt-19.pt"
DATA_DIR = "C:\\Users\\myqkr\\Desktop\\SignalLM\\pkl_data\\test"
LLM_MODEL = "Qwen/Qwen-0.6B"
DEVICE = "cpu"

# ===== Main Execution =====
if __name__ == "__main__":
    # 1. Load VQ model
    print("ğŸ”„ Loading VQ model...")
    vq_model = load_vq_model(VQ_CHECKPOINT, device=DEVICE, weights_only=False)
    print(f"âœ… VQ model loaded from {VQ_CHECKPOINT}\n")
    
    # 2. Get all pkl files
    files = glob.glob(os.path.join(DATA_DIR, "*.pkl"))
    print(f"ğŸ“‚ Found {len(files)} files in {DATA_DIR}\n")
    
    if len(files) == 0:
        print("âŒ No pkl files found!")
        exit(1)
    
    # 3. Load LLM model
    print("ğŸ”„ Loading LLM model...")
    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)
    llm_model = AutoModelForCausalLM.from_pretrained(LLM_MODEL)
    print(f"âœ… LLM model loaded: {LLM_MODEL}\n")
    
    # 4. Statistics
    correct = 0
    total = 0
    results = []
    
    # 5. Process each file
    for idx, filename in enumerate(files, 1):
        print("="*80)
        print(f"ğŸ“„ Processing [{idx}/{len(files)}]: {os.path.basename(filename)}")
        print("="*80)
        
        # Extract tokens
        token_string = get_token_string(vq_model, filename, identifier="TOK")
        label = get_label_from_filename(os.path.basename(filename))
        
        # Create conversation
        conversation = [
            # Round 1: íƒœìŠ¤í¬ ì„¤ëª…
            {
                "role": "user",
                "content": "ì•„ë‚ ë¡œê·¸ ì „ìê¸° ì‹ í˜¸ë¥¼ VQ-VAEë¥¼ ì‚¬ìš©í•´ì„œ í† í°í™”ë¥¼ í–ˆë‹¤. ì „ì²´ 20ì´ˆ ì§œë¦¬ ì‹ í˜¸ë¥¼ 0.1ì´ˆ ë‹¨ìœ„ë¡œ ë‚˜ëˆ ì„œ, í•˜ë‚˜ì˜ í† í°ì´ ë˜ë„ë¡ í•˜ì—¬ ì´ 200ê°œì˜ í† í° ì‹œí€€ìŠ¤ê°€ ìˆë‹¤. ì´ í† í° ì‹œí€€ìŠ¤ë¥¼ ë¶„ì„í•´ì„œ ì‹ í˜¸ì˜ ë ˆì´ë¸”-ì •ìƒ í˜¹ì€ ë¹„ì •ìƒ-ì„ ì œë¡œìƒ·ìœ¼ë¡œ íƒì§€í•´ì•¼ í•˜ëŠ”ë°, ê·¸ ë°©ë²•ì„ ì´ì œë¶€í„° ì•Œë ¤ì¤„ê²Œ."
            },
            {
                "role": "assistant",
                "content": "ì‘. ì‹ í˜¸ í† í° ë¶„ì„í•´ì„œ ì •ìƒ í˜¹ì€ ë¹„ì •ìƒìœ¼ë¡œ ë¶„ë¥˜í•˜ê² ìŠµë‹ˆë‹¤."
            },
            
            # Round 2: íŒì • ê¸°ì¤€ ì„¤ëª…
            {
                "role": "user",
                "content": "ì •ìƒì‹ í˜¸ëŠ” <TOK_257>, <TOK_390>, ...ê³¼ ê°™ì€ í† í°ì´ ìì£¼ ë‚˜íƒ€ë‚˜. ì´ê²Œ ìì£¼ ë“±ì¥í•˜ëŠ” í† í°ì¸ë°, ì´ í† í°ì€ ì •ìƒ ì‹ í˜¸ì™€ ë¹„ì •ìƒ ì‹ í˜¸ ëª¨ë‘ì—ì„œ ê³µí†µë˜ê²Œ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” í† í°ì´ì•¼. ê·¸ëŸ°ë° ë°˜ëŒ€ë¡œ, ìì£¼ ë“±ì¥í•˜ì§€ ì•ŠëŠ” í† í°ì€ ì •ìƒì‹ í˜¸ì—ì„œë§Œ ë‚˜íƒ€ë‚˜. ë‹¤ì‹œë§í•´ì„œ, ì •ìƒì‹ í˜¸ì—ì„œ ì¶œí˜„ë¹ˆë„ê°€ ë‚®ì€ í† í°ë“¤ì´ ë“±ì¥í•œë‹¤ë©´ ê·¸ í† í° ì‹œí€€ìŠ¤ëŠ” ì •ìƒ ì‹ í˜¸ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§€ê³ , ê·¸ í† í°ë“¤ì´ ë“±ì¥í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ë¹„ì •ìƒ ì‹ í˜¸ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§€ëŠ”ê±°ì•¼."
            },
            {
                "role": "assistant",
                "content": "ì‘ ê³ ë§ˆì›Œ. ê·¸ë ‡ë‹¤ë©´ ì´ì œ ì‹ í˜¸ë¥¼ ë¶„ì„í•´ë³¼ê¹Œ?"
            },
            
            # Round 3: ì‹¤ì œ ë¶„ì„ ìš”ì²­
            {
                "role": "user",
                "content": f"ì‘, ì´ì œ í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë‹¬í• ê²Œ.\n\n{create_prompt(token_string)}"
            }
        ]
        
        # Generate prompt with chat template
        prompt = tokenizer.apply_chat_template(
            conversation, 
            tokenize=False,
            add_generation_prompt=True
        )
        
        # Inference
        inputs = tokenizer(prompt, return_tensors="pt")
        outputs = llm_model.generate(**inputs, max_new_tokens=200, do_sample=True, temperature=0.7)
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Parse label
        predicted_label = parse_label_from_response(response)
        
        # Check correctness
        is_correct = (predicted_label == label.upper())
        if is_correct:
            correct += 1
        total += 1
        
        # Store result
        results.append({
            "filename": os.path.basename(filename),
            "true_label": label,
            "predicted_label": predicted_label,
            "correct": is_correct
        })
        
        # Print result
        print(f"ğŸ¤– Predicted: {predicted_label}")
        print(f"âœ… True Label: {label}")
        print(f"{'âœ… CORRECT!' if is_correct else 'âŒ WRONG'}")
        print()
    
    # 6. Print summary
    print("="*80)
    print("ğŸ“Š FINAL RESULTS")
    print("="*80)
    print(f"Total Files: {total}")
    print(f"Correct: {correct}")
    print(f"Wrong: {total - correct}")
    print(f"Accuracy: {correct/total*100:.2f}%")
    print("="*80)
    
    # 7. Print detailed results
    print("\nğŸ“‹ Detailed Results:")
    print("-"*80)
    for result in results:
        status = "âœ…" if result["correct"] else "âŒ"
        print(f"{status} {result['filename']}: {result['true_label']} -> {result['predicted_label']}")
    print("="*80)