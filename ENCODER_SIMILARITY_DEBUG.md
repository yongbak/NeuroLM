# Encoder Similarity ì§„ë‹¨ ë° ëŒ€ì‘ ê°€ì´ë“œ

## ğŸ”¬ í˜„ì¬ ì¸¡ì • ë°©ë²• ìš”ì•½

### ë¬´ì—‡ì„ ì¸¡ì •í•˜ëŠ”ê°€?
**ì„œë¡œ ë‹¤ë¥¸ í† í°ë“¤ ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í‰ê· **

```python
# ê³„ì‚° ê³¼ì •:
1. encoder_features: (batch=32, tokens=200, dim=768)
2. Flatten â†’ (6400, 768)  # 6400ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í† í°
3. L2 Normalize â†’ ë‹¨ìœ„ ë²¡í„°ë¡œ ë³€í™˜
4. Random sampling â†’ 100ê°œ í† í° ì„ íƒ
5. Pairwise similarity â†’ 100x100 í–‰ë ¬
6. ëŒ€ê°ì„  ì œì™¸ í‰ê·  â†’ avg_similarity
```

### ë¹„êµ ëŒ€ìƒ
- **í† í° A**: ìƒ˜í”Œ1, ìœ„ì¹˜10, ì‹œê°„êµ¬ê°„ 2000~2200
- **í† í° B**: ìƒ˜í”Œ15, ìœ„ì¹˜150, ì‹œê°„êµ¬ê°„ 30000~30200  
- **í† í° C**: ìƒ˜í”Œ28, ìœ„ì¹˜77, ì‹œê°„êµ¬ê°„ 15400~15600

â†’ **ì™„ì „íˆ ë‹¤ë¥¸ ìƒ˜í”Œ, ë‹¤ë¥¸ ìœ„ì¹˜, ë‹¤ë¥¸ ì‹œê°„ì˜ í† í°ë“¤ë¼ë¦¬ ë¹„êµ**

---

## ğŸš¨ ë†’ì€ Similarityì˜ ì˜ë¯¸

### Similarity > 0.95
```
ğŸ”´ CRITICAL: ì¸ì½”ë” ì¶œë ¥ ë¶•ê´´ (Encoder Collapse)

ì›ì¸:
- Encoderê°€ ì…ë ¥ê³¼ ë¬´ê´€í•˜ê²Œ ê±°ì˜ ë™ì¼í•œ ë²¡í„° ì¶œë ¥
- Weight initialization ë¬¸ì œ
- Gradient vanishing/exploding
- í•™ìŠµë¥ ì´ ë„ˆë¬´ ë†’ê±°ë‚˜ ë‚®ìŒ
- Batch normalization ë¬¸ì œ

ê²°ê³¼:
- ëª¨ë“  ì…ë ¥ì´ ê°™ì€ ì½”ë“œë¶ìœ¼ë¡œ ë§¤í•‘ë¨
- Codebook collapseì˜ ê·¼ë³¸ ì›ì¸
- Reconstruction ë¶ˆê°€ëŠ¥
```

### Similarity 0.85~0.95
```
ğŸŸ¡ WARNING: ë‹¤ì–‘ì„± ë¶€ì¡±

ì›ì¸:
- Encoder capacity ë¶€ì¡± (layer/dim ë„ˆë¬´ ì‘ìŒ)
- Overfitting to dominant patterns
- Data augmentation ë¶€ì¡±
- Position encoding ë¬¸ì œ

ì¡°ì¹˜:
- DECAY ë‚®ì¶”ê¸° (0.9 â†’ 0.7)
- Dropout ì¶”ê°€/ì¦ê°€
- Data augmentation ê°•í™”
```

### Similarity 0.65~0.85
```
ğŸŸ¢ NORMAL: ì •ìƒ ë²”ìœ„

ì´ìœ :
- ê°™ì€ ë„ë©”ì¸(EEG)ì´ë¯€ë¡œ ì–´ëŠì •ë„ ìœ ì‚¬ì„± ìì—°ìŠ¤ëŸ¬ì›€
- ë‡ŒíŒŒëŠ” íŠ¹ì • íŒ¨í„´(alpha, beta wave ë“±) ë°˜ë³µ
- ê±´ê°•í•œ ë‹¤ì–‘ì„± ìœ ì§€
```

### Similarity < 0.65
```
ğŸŸ¢ EXCELLENT: ë§¤ìš° ë‹¤ì–‘í•¨

- ì´ìƒì ì¸ ìƒíƒœ
- Encoderê°€ ì…ë ¥ì˜ ë¯¸ì„¸í•œ ì°¨ì´ë„ ì˜ êµ¬ë¶„
- Codebook í™œìš©ë„ ë†’ì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€
```

---

## ğŸ”§ ë¬¸ì œ í•´ê²° ë°©ë²•

### 1ë‹¨ê³„: ì›ì¸ ì§„ë‹¨

```bash
# Training ë¡œê·¸ í™•ì¸
grep "Encoder Diversity" train.log | tail -20

# íŒ¨í„´ ë¶„ì„:
# - ê°‘ìê¸° ì˜¬ë¼ê°: í•™ìŠµ ì¤‘ ë¬¸ì œ ë°œìƒ (gradient explosion?)
# - ì²˜ìŒë¶€í„° ë†’ìŒ: Initialization ë¬¸ì œ
# - ì„œì„œíˆ ì˜¬ë¼ê°: Overfitting or collapse ì§„í–‰ ì¤‘
```

### 2ë‹¨ê³„: Feature Std í•¨ê»˜ í™•ì¸

```python
# Feature stdê°€ í•¨ê»˜ ë–¨ì–´ì§€ë©´ í™•ì‹¤í•œ collapse
if avg_similarity > 0.9 and feature_std < 0.01:
    print("ğŸ”´ í™•ì‹¤í•œ Encoder Collapse!")
    print("   â†’ Encoder ì¬ì´ˆê¸°í™” ë˜ëŠ” architecture ë³€ê²½ í•„ìš”")
```

### 3ë‹¨ê³„: ì¦‰ê° ëŒ€ì‘

#### Option A: Encoder Initialization ì¬ì„¤ì •
```python
# model/model_neural_transformer.pyì—ì„œ

class NeuralTransformer(nn.Module):
    def __init__(self, config):
        super().__init__()
        # ...
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            # ë” ì‘ì€ stdë¡œ ì´ˆê¸°í™”
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.01)  # 0.02 â†’ 0.01
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
```

#### Option B: Dropout ì¶”ê°€
```python
# encoder_configì— dropout ì„¤ì •
encoder_args = dict(
    n_layer=8,
    n_head=8,
    n_embd=768,
    dropout=0.2,  # 0.0 â†’ 0.2ë¡œ ì¦ê°€
    bias=False,
)
```

#### Option C: Learning Rate ì¡°ì •
```bash
# ë„ˆë¬´ ë†’ìœ¼ë©´ ë°œì‚°, ë„ˆë¬´ ë‚®ìœ¼ë©´ collapse
python train_vq.py \
    --learning_rate 1e-5 \  # 5e-5 â†’ 1e-5ë¡œ ë‚®ì¶¤
    --warmup_epochs 20      # 10 â†’ 20ìœ¼ë¡œ ì¦ê°€
```

#### Option D: Gradient Clipping ê°•í™”
```bash
python train_vq.py \
    --grad_clip 1.0  # 0.0 â†’ 1.0
```

#### Option E: Batch Size ì¡°ì •
```bash
# ë„ˆë¬´ ì‘ì€ batch sizeëŠ” ë¶ˆì•ˆì •
python train_vq.py \
    --batch_size 32      # 4 â†’ 32
    --gradient_accumulation_steps 4
```

---

## ğŸ“Š ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

### Training ì¤‘ í™•ì¸ ì‚¬í•­

```python
# ë§¤ 10 iterationsë§ˆë‹¤ ì¶œë ¥:
ğŸ”¬ Encoder Diversity (iter 50):
  Avg similarity: 0.7234 (1.0=identical, 0.0=orthogonal)
  Feature std: 0.1456 (0.0=collapsed)

# ì •ìƒ íŒ¨í„´:
# iter 10:  0.75, std 0.14
# iter 50:  0.72, std 0.15
# iter 100: 0.69, std 0.16  â†’ ì ì  ë‹¤ì–‘í•´ì§ (good!)

# ë¬¸ì œ íŒ¨í„´:
# iter 10:  0.75, std 0.14
# iter 50:  0.85, std 0.08
# iter 100: 0.95, std 0.02  â†’ ì ì  ë¶•ê´´ë¨ (bad!)
```

### ì¦‰ì‹œ ì¤‘ë‹¨ ê¸°ì¤€
```python
if avg_similarity > 0.95 and feature_std < 0.01:
    print("ğŸ›‘ STOP TRAINING!")
    print("   Encoder has collapsed. Restart with different hyperparameters.")
    # Training ì¤‘ë‹¨í•˜ê³  ì„¤ì • ë³€ê²½
```

---

## ğŸ§ª ë””ë²„ê¹… ì½”ë“œ

í˜„ì¬ ìƒí™©ì„ ë” ìì„¸íˆ íŒŒì•…í•˜ë ¤ë©´:

```python
# train_vq.pyì— ì„ì‹œë¡œ ì¶”ê°€:
if DEBUG_ENCODER and iter_num % 10 == 0:
    # ê¸°ì¡´ ì½”ë“œ...
    
    # ì¶”ê°€ ì§„ë‹¨:
    # 1. ë°°ì¹˜ë³„ similarity
    batch_sims = []
    for b in range(encoder_features.size(0)):
        batch_tokens = encoder_features[b]  # (tokens, dim)
        batch_norm = F.normalize(batch_tokens, p=2, dim=-1)
        batch_sim = torch.mm(batch_norm, batch_norm.t())
        mask = ~torch.eye(batch_sim.size(0), dtype=torch.bool, device=batch_sim.device)
        batch_sims.append(batch_sim[mask].mean().item())
    
    print(f"  Per-sample similarity: {torch.tensor(batch_sims).mean():.4f} Â± {torch.tensor(batch_sims).std():.4f}")
    
    # 2. ì²«ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ í† í°ì˜ similarity
    first_tokens = encoder_features[:, 0, :]  # (batch, dim)
    last_tokens = encoder_features[:, -1, :]   # (batch, dim)
    first_norm = F.normalize(first_tokens, p=2, dim=-1)
    last_norm = F.normalize(last_tokens, p=2, dim=-1)
    positional_sim = (first_norm * last_norm).sum(dim=-1).mean().item()
    print(f"  First-Last token similarity: {positional_sim:.4f}")
    
    # 3. ê°œë³„ í† í°ì˜ norm í™•ì¸
    token_norms = encoder_features.norm(dim=-1).mean().item()
    print(f"  Avg token norm: {token_norms:.4f}")
```

---

## ğŸ¯ Target Metrics

### ê±´ê°•í•œ Encoderì˜ ì§€í‘œ
```
Avg similarity:  0.60 ~ 0.80
Feature std:     0.05 ~ 0.20
Token norm:      5.0 ~ 15.0 (normalize ì „)
Codebook usage:  > 60%
```

### Collapse ì§•í›„
```
Avg similarity:  > 0.90
Feature std:     < 0.02
Token norm:      ë§¤ìš° í¬ê±°ë‚˜ ì‘ìŒ (< 1.0 or > 100)
Codebook usage:  < 20%
```

---

## ğŸ’¡ Similarityê°€ ê°‘ìê¸° ë†’ì•„ì§„ ê²½ìš°

### ì¦‰ì‹œ ì²´í¬ë¦¬ìŠ¤íŠ¸:

1. **Learning rate í™•ì¸**
   ```bash
   # Warmup ëë‚¬ëŠ”ì§€ í™•ì¸
   # iter_numê³¼ warmup_steps ë¹„êµ
   ```

2. **Gradient norm í™•ì¸**
   ```bash
   # Lossê°€ NaNì´ê±°ë‚˜ infinityì¸ì§€
   # Gradient explosion ê°€ëŠ¥ì„±
   ```

3. **ìµœê·¼ ë³€ê²½ì‚¬í•­ ì²´í¬**
   ```bash
   # Dead code reset ì ìš© í›„ì¸ì§€?
   # DECAY/BETA ë³€ê²½í–ˆëŠ”ì§€?
   # Checkpointì—ì„œ resumeí–ˆëŠ”ì§€?
   ```

4. **Data í™•ì¸**
   ```bash
   # í˜¹ì‹œ ê°™ì€ ë°°ì¹˜ê°€ ë°˜ë³µë˜ëŠ”ì§€
   # Data augmentationì´ êº¼ì¡ŒëŠ”ì§€
   ```

---

## ğŸ“ ìš”ì•½

**Avg Similarity = ëœë¤í•˜ê²Œ ì„ íƒí•œ 100ê°œ í† í°ë“¤ ê°„ì˜ í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„**

- **ë¹„êµ ëŒ€ìƒ**: ì„œë¡œ ë‹¤ë¥¸ ìƒ˜í”Œ, ë‹¤ë¥¸ ìœ„ì¹˜ì˜ í† í°ë“¤
- **ì •ìƒ ë²”ìœ„**: 0.65 ~ 0.85
- **ë¬¸ì œ ì§•í›„**: > 0.90 (íŠ¹íˆ feature_std < 0.02ì¼ ë•Œ)
- **ëŒ€ì‘**: Learning rate ë‚®ì¶”ê¸°, Dropout ì¶”ê°€, Initialization ì¬ê²€í† 

**ê°‘ìê¸° ë†’ì•„ì¡Œë‹¤ë©´**: í•™ìŠµ ì¤‘ ë¬¸ì œ ë°œìƒ â†’ ì¦‰ì‹œ ì›ì¸ íŒŒì•… í•„ìš”!
